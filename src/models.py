import warnings

import numpy as np
from abcpy.continuousmodels import ProbabilisticModel, Continuous, InputConnector, Uniform, Normal
from abcpy.statistics import Statistics, Identity
from scipy.optimize import fsolve
from scipy.stats import multivariate_normal
from statsmodels.tsa.arima_process import arma_generate_sample

from src.utils import define_exact_param_values, transform_R_to_theta_MA2


def ma2_log_lik_for_mcmc(theta, x_obs):
    return ma2_likelihood_for_R_variables(x_obs, theta[0], theta[1])


def ma2_likelihood_for_R_variables(x, R1, R2):
    theta1, theta2 = transform_R_to_theta_MA2(R1, R2)

    return ma2_likelihood(x, theta1, theta2)


def ma2_likelihood(x, theta_1, theta_2):
    """x here is a 2d array: first index is the simulation number (iid draw), second is the components"""
    cov_matrix = np.zeros([x.shape[-1]] * 2)
    a = 1 + theta_1 ** 2 + theta_2 ** 2
    cov_matrix[0, 0] = 1
    cov_matrix[1, 1] = 1 + theta_1 ** 2
    b = theta_1 * (1 + theta_2)
    cov_matrix[0, 1] = theta_1
    cov_matrix[1, 0] = theta_1
    for i in range(2, x.shape[-1]):
        cov_matrix[i, i] = a
        cov_matrix[i - 1, i] = b
        cov_matrix[i, i - 1] = b
        cov_matrix[i - 2, i] = theta_2
        cov_matrix[i, i - 2] = theta_2
    return np.sum(multivariate_normal.logpdf(x, cov=cov_matrix))


# these come from abcpy-models
class ARMAmodel(ProbabilisticModel, Continuous):

    def __init__(self, parameters, num_AR_params=2, num_MA_params=2, size=100, name='ARMA model'):
        """size is the length of the timeseries to be generated by the model.
        The AR parameters always need to be passed before the MA parameters."""

        if not isinstance(parameters, list):
            raise TypeError('Input of ARMAmodel model is of type list')

        self.size = size
        self.num_AR_params = num_AR_params
        self.num_MA_params = num_MA_params
        self.total_num_params = num_AR_params + num_MA_params

        self._check_num_parameters(parameters)

        input_connector = InputConnector.from_list(parameters)
        super(ARMAmodel, self).__init__(input_connector, name)

    def _check_input(self, input_values):
        # Check whether input has correct type or format
        if len(input_values) != self.total_num_params:
            return False
        return True

    def _check_num_parameters(self, input_values):
        # Check whether input has correct type or format
        if len(input_values) != self.total_num_params:
            raise RuntimeError(
                'Input list must be of length {}, containing {} AR parameters and {} MA parameters'.format(
                    self.total_num_params, self.num_AR_params, self.num_MA_params))
        return True

    def _check_output(self, values):
        # f not isinstance(values[0], np.ndarray):
        #   raise ValueError('Output of the normal distribution is always a number.')
        return True

    def get_output_dimension(self):
        return self.size

    def get_number_parameters(self):
        return self.total_num_params

    def forward_simulate(self, input_values, k, rng=np.random.RandomState()):

        self._check_num_parameters(input_values)

        results = []
        arparams = np.array(input_values[0:self.num_AR_params])
        maparams = np.array(input_values[self.num_AR_params:self.num_AR_params + self.num_MA_params])
        ar = np.r_[1, -arparams.squeeze()]  # add zero-lag and negate
        ma = np.r_[1, maparams.squeeze()]  # add zero-lag

        for i in range(k):
            # use the random number generators provided as a way to draw the random gaussians. This is for
            # reproducibility. It does not work for statsmodels after 0.9.0
            results.append(arma_generate_sample(ar, ma, self.size, distrvs=rng.randn))

            if np.any(np.isinf(results[-1])):
                warnings.warn("Infinity in forward ARMA model", RuntimeWarning)
            if np.any(np.isnan(results[-1])):
                warnings.warn("nan in forward ARMA model", RuntimeWarning)

        return results


class MA2Reparametrized(ARMAmodel):

    def __init__(self, parameters, size=100, name='MA2Reparametrized'):
        """size is the length of the timeseries to be generated by the model. Parameter inputs to this model are
        R1 and R2 variables, from which theta1, theta2 are obtained.

        We in fact consider triangular region as specified in Marin et al (2012); we sample from it using the trick
        in https://math.stackexchange.com/questions/18686/uniform-random-point-in-triangle (implemented in
        self._transform_R_to_theta). With R1 and R2 uniform in [0,1], you obtain theta1, theta2 uniform on the correct
        triangular region.
        """

        self.size = size

        ARMAmodel.__init__(self, parameters, num_AR_params=0, num_MA_params=2, size=size, name=name)

    def forward_simulate(self, input_values, num_forward_simulations, rng=np.random.RandomState()):
        R1 = input_values[0]
        R2 = input_values[1]

        theta1, theta2 = self._transform_R_to_theta(R1, R2)

        return ARMAmodel.forward_simulate(self, [theta1, theta2], k=num_forward_simulations, rng=rng)

    def _check_input(self, input_values):
        """Check whether theta1 and theta2 from the transformation satisfy the inequalities:"""

        # first check number of parameters:
        if not ARMAmodel._check_input(self, input_values):
            return False

        # then check inequalities:
        R1 = input_values[0]
        R2 = input_values[1]

        theta1, theta2 = self._transform_R_to_theta(R1, R2)
        if not (-1 < theta2 < 1 and theta1 + theta2 > -1 or theta1 - theta2 < 1):
            return False

        return True

    @staticmethod
    def _transform_R_to_theta(R1, R2):
        R1_sqrt = R1 ** 0.5
        theta1 = ((4 - 2 * R2) * R1_sqrt - 2)
        theta2 = (1 - 2 * R2 * R1_sqrt)
        return theta1, theta2


class Multivariate_g_and_k(ProbabilisticModel, Continuous):
    """
    Multivariate g_and_k: a multivariate random normal with mean 0 and covariance matrix with 1 elements on the
    diagonal and rho on the upper and lower secondary diagonal is drawn. Then, the g-and-k transformation is
    applied to obtain the multivariate g-and-k.

    When using one single dimension, this is the standard g-and-k distribution.

    See for instance Section 4.5 of [1] for a discussion of the multivariate g-and-k in an ABC setup.

    This implementation was inspired by the R code at https://github.com/dennisprangle/gk, released under
    the GPL-2.0 License. The corresponding R package is discussed in [2], which also contains details on g-and-k
    distribution.

    [1] Jiang, Bai. "Approximate Bayesian computation with Kullback-Leibler divergence as data discrepancy."
    International Conference on Artificial Intelligence and Statistics. PMLR, 2018.
    [2] Prangle, Dennis. "gk: An R Package for the g-and-k and Generalised g-and-h Distributions."
    arXiv preprint arXiv:1706.06889 (2017), url https://arxiv.org/abs/1706.06889.
    """

    def __init__(self, parameters, size=5, name='Multivariate_g_and_k'):

        self.size = size
        self.c = 0.8  # fix this
        input_parameters = InputConnector.from_list(parameters)
        super(Multivariate_g_and_k, self).__init__(input_parameters, name)

    def forward_simulate(self, input_values, num_forward_simulations, rng=np.random.RandomState()):
        A = input_values[0]
        B = input_values[1]
        g = input_values[2]
        k = input_values[3]
        rho = input_values[4]

        result = self._draw_multiv_g_and_k(num_forward_simulations, self.size, A, B, g, k, rho, c=self.c, rng=rng)

        return [x for x in result]

    def _create_cov_matrix(self, dim, rho):
        return np.eye(dim) + rho * np.eye(dim, k=1) + rho * np.eye(dim, k=-1)

    def _draw_multiv_g_and_k(self, n, dim, A, B, g, k, rho, c=0.8, rng=np.random.RandomState()):
        """n is the number of samples, dim is the dimensions"""
        # define the covariance matrix first:

        cov = self._create_cov_matrix(dim, rho)

        z = rng.multivariate_normal(mean=np.zeros(dim), cov=cov, size=n)
        return self._z2gk(z, A, B, g, k, c=c)

    @staticmethod
    def _z2gk(z, A, B, g, k, c=0.8):
        """Transform a sample from a standard normal (z) to a g-and-k draw. This assumes z, A, B, g, k and c are
        scalars, while z can be an array. This is basically the Q function (the inverse of cdf) in the standard
        notation"""

        z = np.atleast_1d(z)
        z_squared = z ** 2
        if g == 0:
            term1 = 1
        else:
            term1 = (1 + c * np.tanh(g * z * 0.5))
        term2 = np.zeros_like(z, dtype=float)
        zbig = np.isinf(z_squared)
        zsmall = np.logical_not(zbig)
        term2[zbig] = np.sign(z[zbig]) * np.abs(z[zbig]) ** (1 + 2 * k)
        term2[zsmall] = z[zsmall] * (1 + z_squared[zsmall]) ** k

        return A + B * term1 * term2

    @staticmethod
    def _Q_log_derivative(z, A, B, g, k, c=0.8):
        """Compute the derivative of the Q function (the inverse cumulative function) evaluated at z."""

        z = np.atleast_1d(z)
        z_squared = z ** 2

        # These are used to correct edge cases
        # (likely to be rare so no need for particularly efficient code)
        zbig = np.isinf(z_squared)
        zsmall = np.logical_not(zbig)
        term1 = np.zeros_like(z, dtype=float)
        if k == 0:
            term1 = 0
        else:
            term1[zsmall] = k * np.log(1 + z_squared[zsmall])
            term1[zbig] = 2 * k * np.log(abs(z[zbig]))
        if g == 0:
            term2 = 1
            term4 = 0
        else:
            gz = g * z
            term2 = 1 + c * np.tanh(gz / 2)
            term4 = c * gz / (2 * np.cosh(gz / 2) ** 2)

        term3 = np.zeros_like(z, dtype=float)
        term3[zsmall] = (1 + (2 * k + 1) * z_squared[zsmall]) / (1 + z_squared[zsmall])
        term3[zbig] = 2 * k + 1
        # term4[ is.infinite(z)] = 0

        return np.log(B) + term1 + np.log(term2 * term3 + term4)

    def get_output_dimension(self):
        return self.size

    def _check_input(self, input_values):
        """
        It is standard to take B > 0 and fix c = 0.8, and in this case k ≥ 0 guarantees a proper distribution.
        """
        if len(input_values) != 5:
            return False
        A = input_values[0]
        B = input_values[1]
        g = input_values[2]
        k = input_values[3]
        rho = input_values[4]

        if not (-1 <= rho <= 1):
            return False
        if B <= 0 or k < 0:
            return False

        return True

    def _check_output(self, values):
        return True

    def logpdf(self, x, input_values, **kwargs_fsolve):
        """x here can be a single (multivariate) observation (1d array) or a set of observations (2d) array, with
         second index denoting the components"""
        if len(x.shape) not in (1, 2):
            raise RuntimeError("Incorrect number of components for x")

        A = input_values[0]
        B = input_values[1]
        g = input_values[2]
        k = input_values[3]
        rho = input_values[4]

        # first: solve the equation x_j = Q(z_j; theta) for z_j, for all components of x; this requires solving
        # numerically. Check how they do in g and k package.
        z = np.zeros_like(x.flatten())
        for j, x_j in enumerate(x.flatten()):
            func = lambda z: self._z2gk(z, A, B, g, k, self.c) - x_j

            z[j] = fsolve(func, x0=np.array([0]), **kwargs_fsolve)
        z = z.reshape(x.shape)

        # could also do it with one single call -> faster. Not sure if that works however in the same way, as the
        # scaling may be dependent on the dimension

        # func = lambda z: self._z2gk(z, A, B, g, k, self.c) - x
        # z = fsolve(func, x0=np.zeros_like(x), args=())
        # print(np.allclose(z, z2))

        # second: compute the log pdf by using the multivariate normal pdf and the derivative of the quantile function
        # evaluated in z_j
        cov = self._create_cov_matrix(self.size, rho)
        logpdf_mvn = multivariate_normal.logpdf(z, mean=np.zeros(self.size), cov=cov, allow_singular=True)

        return np.sum(logpdf_mvn) - np.sum(self._Q_log_derivative(z, A, B, g, k, self.c))


class Univariate_g_and_k(Multivariate_g_and_k):
    """Alias the multivariate one for the univariate case """

    def __init__(self, parameters, name='Univariate_g_and_k'):
        rho = 0  # unused
        super(Univariate_g_and_k, self).__init__(parameters + [rho], size=1, name=name)

    def forward_simulate(self, input_values, num_forward_simulations, rng=np.random.RandomState()):
        rho = 0  # this value is needed only for the parent method but unused with size=1
        return super(Univariate_g_and_k, self).forward_simulate(input_values + [rho], num_forward_simulations,
                                                                rng=rng)


class MG1Queue(ProbabilisticModel, Continuous):
    """Simulates a M/G/1 queueing system with Uni[theta1, theta2] service times and Exp(theta3) interarrival times.
    It returns the interdeparture time for the first number_steps steps, assuming the queue starts empty.

    [1] Nelson, Barry. Foundations and methods of stochastic simulation: a first course. Springer Science & Business
    Media, 2013.
    """

    def __init__(self, parameters, number_steps=5, name='M/G/1'):

        self.number_steps = number_steps
        input_parameters = InputConnector.from_list(parameters)
        super(MG1Queue, self).__init__(input_parameters, name)

    def forward_simulate(self, input_values, num_forward_simulations, rng=np.random.RandomState()):
        theta1 = input_values[0]
        theta2 = input_values[1]
        theta3 = input_values[2]

        simulations = self.simulate_mg1_vectorized(theta1, theta2, theta3, rng, num_forward_simulations)
        result = [None] * num_forward_simulations
        for i in range(num_forward_simulations):
            result[i] = simulations[:, i]

        return result

    def simulate_mg1_single(self, theta1, theta2, theta3, rng):
        # use here Lindley equation (notation follows chapter 4.3 of [1]) .
        Y = np.zeros(self.number_steps + 1)
        X = np.zeros(self.number_steps + 1)
        A = np.zeros(self.number_steps + 1)
        inter_dep = np.zeros(self.number_steps + 1)

        for i in range(1, self.number_steps + 1):
            A[i] = rng.exponential(scale=1 / theta3)  # scale is 1/rate
            X[i] = rng.uniform(theta1, theta2)
            Y[i] = np.max([0, Y[i - 1] + X[i - 1] - A[i]])
            # compute the inter-departure times:
            inter_dep[i] = A[i] + X[i] + Y[i] - Y[i - 1] - X[i - 1]
        # print(Y)
        return inter_dep[1:]

    def simulate_mg1_vectorized(self, theta1, theta2, theta3, rng, num_forward_simulations=1):
        # use here Lindley equation (notation follows chapter 4.3 of [1]) .
        Y = np.zeros((self.number_steps + 1, num_forward_simulations))
        X = np.zeros((self.number_steps + 1, num_forward_simulations))
        A = np.zeros((self.number_steps + 1, num_forward_simulations))
        inter_dep = np.zeros((self.number_steps + 1, num_forward_simulations))

        for i in range(1, self.number_steps + 1):
            A[i] = rng.exponential(scale=1 / theta3, size=num_forward_simulations)  # scale is 1/rate
            X[i] = rng.uniform(theta1, theta2, size=num_forward_simulations)
            Y[i] = np.clip(Y[i - 1] + X[i - 1] - A[i], a_min=0, a_max=np.infty)  # clip from below with 0
            # compute the inter-departure times:
            inter_dep[i] = A[i] + X[i] + Y[i] - Y[i - 1] - X[i - 1]
        return inter_dep[1:]

    def get_output_dimension(self):
        return self.number_steps

    def _check_input(self, input_values):
        """
        """
        if len(input_values) != 3:
            return False

        if input_values[0] < 0 or input_values[1] <= 0 or input_values[2] <= 0 or input_values[0] > input_values[1]:
            return False
        return True

    def _check_output(self, values):
        return True


class MG1QueueReparametrized(MG1Queue):
    """This takes as input theta1, theta2-theta1, theta3 and then calls the MG1Queue model"""

    def forward_simulate(self, input_values, num_forward_simulations, rng=np.random.RandomState()):
        theta1 = input_values[0]
        theta2_minus_theta1 = input_values[1]
        theta3 = input_values[2]

        theta2 = theta2_minus_theta1 + theta1

        return MG1Queue.forward_simulate(self, [theta1, theta2, theta3], num_forward_simulations, rng=rng)

    def _check_input(self, input_values):
        """
        """
        theta1 = input_values[0]
        theta2_minus_theta1 = input_values[1]
        theta3 = input_values[2]

        return MG1Queue._check_input(self, [theta1, theta2_minus_theta1 + theta1, theta3])


class LogStatistics(Statistics):
    """Element-wise log of the data
    """

    def __init__(self):
        pass

    def statistics(self, data):
        if isinstance(data, list):
            if np.array(data).shape == (len(data),):
                if len(data) == 1:
                    data = np.array(data).reshape(1, 1)
                data = np.array(data).reshape(len(data), 1)
            else:
                data = np.concatenate(data).reshape(len(data), -1)
        else:
            raise TypeError('Input data should be of type list, but found type {}'.format(type(data)))

        return np.log(data)


def instantiate_model(model_name, reparametrized=True, **kwargs):
    param_bounds = None
    statistics = Identity(degree=1, cross=False)

    if model_name == "MA2":
        if "size" in kwargs.keys():
            size = kwargs["size"]
        else:
            size = 50

        if reparametrized:
            R1 = Uniform([[0], [1]], name='R1')
            R2 = Uniform([[0], [1]], name='R2')

            model = MA2Reparametrized([R1, R2], size=size, **kwargs)
            # Notice however that here the journal stores R1 and R2, not theta1 and theta2! Need then to convert to
            # those in order to obtain the exact posterior samples...
            param_bounds = {"R1": [0, 1], "R2": [0, 1]}
        else:
            # instantiate the non-reparametrized version for simulations from a specified parameter value:
            par_values = define_exact_param_values()[model_name]

            theta1 = par_values[0]
            theta2 = par_values[1]

            model = ARMAmodel([theta1, theta2], num_AR_params=0, num_MA_params=2, size=size)

    elif model_name in ["g-and-k", "Cauchy_g-and-k"]:
        # prior ranges as in Jiang et al (2018) and Fujisawa et al (2020)
        A = Uniform([[0], [4]], name='A')
        B = Uniform([[0], [4]], name='B')
        g = Uniform([[0], [4]], name='g')
        k = Uniform([[0], [4]], name='k')
        rho_bound = np.sqrt(3) / 3
        rho = Uniform([[-rho_bound], [rho_bound]], name='rho')  # notice this had a wrong name

        model = Multivariate_g_and_k([A, B, g, k, rho], **kwargs)
        param_bounds = {'A': [0, 4], 'B': [0, 4], 'g': [0, 4], 'k': [0, 4], "rho": [-rho_bound, rho_bound]}

    elif model_name in ["univariate_g-and-k", "univariate_Cauchy_g-and-k"]:
        # prior ranges as in Jiang et al (2018) and Fujisawa et al (2020)
        A = Uniform([[0], [4]], name='A')
        B = Uniform([[0], [4]], name='B')
        g = Uniform([[0], [4]], name='g')
        k = Uniform([[0], [4]], name='k')
        rho_useless = 0

        model = Multivariate_g_and_k([A, B, g, k, rho_useless], size=1)
        param_bounds = {'A': [0, 4], 'B': [0, 4], 'g': [0, 4], 'k': [0, 4]}

    elif model_name == "MG1":
        # prior ranges as in Jiang et al (2018) and Fujisawa et al (2020)
        if reparametrized:
            # use the parametrized definition for inference routines
            theta1 = Uniform([[0], [10]], name='theta1')
            theta2_minus_theta1 = Uniform([[0], [10]], name='theta2_minus_theta1')  # theta2 - theta1 uniform in [0,10]
            theta3 = Uniform([[0], [1 / 3]], name='theta3')

            model = MG1QueueReparametrized([theta1, theta2_minus_theta1, theta3], number_steps=50, **kwargs)
            param_bounds = {'theta1': [0, 10], 'theta2_minus_theta1': [0, 10], 'theta3': [0, 1 / 3]}
        else:
            # instantiate the non-reparametrized version for simulations from a specified parameter value:
            par_values = define_exact_param_values()[model_name]

            theta1 = par_values[0]
            theta2 = par_values[1]
            theta3 = par_values[2]

            model = MG1Queue([theta1, theta2, theta3], number_steps=50, **kwargs)

        statistics = LogStatistics()

    elif model_name == "normal_location_misspec":

        theta = Normal([[0], [1]], name='theta')
        model = Normal([[theta], [1]], name='normal_location_misspec')

    else:
        raise NotImplementedError

    return model, statistics, param_bounds
