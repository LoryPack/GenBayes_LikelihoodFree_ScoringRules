import warnings

import numpy as np
from abcpy.continuousmodels import ProbabilisticModel, Continuous, InputConnector, Uniform, Normal
from abcpy.statistics import Statistics, Identity
from scipy.optimize import fsolve
from scipy.stats import moment, multivariate_normal, skew, kurtosis  # kurtosis computes the excess kurtosis
from statsmodels.tsa.arima_process import arma_generate_sample

from src.utils import define_exact_param_values, transform_R_to_theta_MA2


def ma2_log_lik_for_mcmc(theta, x_obs):
    return ma2_likelihood_for_R_variables(x_obs, theta[0], theta[1])


def ma2_likelihood_for_R_variables(x, R1, R2):
    theta1, theta2 = transform_R_to_theta_MA2(R1, R2)

    return ma2_likelihood(x, theta1, theta2)


def ma2_likelihood(x, theta_1, theta_2):
    """x here is a 2d array: first index is the simulation number (iid draw), second is the components"""
    cov_matrix = np.zeros([x.shape[-1]] * 2)
    a = 1 + theta_1 ** 2 + theta_2 ** 2
    cov_matrix[0, 0] = 1
    cov_matrix[1, 1] = 1 + theta_1 ** 2
    b = theta_1 * (1 + theta_2)
    cov_matrix[0, 1] = theta_1
    cov_matrix[1, 0] = theta_1
    for i in range(2, x.shape[-1]):
        cov_matrix[i, i] = a
        cov_matrix[i - 1, i] = b
        cov_matrix[i, i - 1] = b
        cov_matrix[i - 2, i] = theta_2
        cov_matrix[i, i - 2] = theta_2
    return np.sum(multivariate_normal.logpdf(x, cov=cov_matrix))


# these come from abcpy-models
class ARMAmodel(ProbabilisticModel, Continuous):

    def __init__(self, parameters, num_AR_params=2, num_MA_params=2, size=100, name='ARMA model'):
        """size is the length of the timeseries to be generated by the model.
        The AR parameters always need to be passed before the MA parameters."""

        if not isinstance(parameters, list):
            raise TypeError('Input of ARMAmodel model is of type list')

        self.size = size
        self.num_AR_params = num_AR_params
        self.num_MA_params = num_MA_params
        self.total_num_params = num_AR_params + num_MA_params

        self._check_num_parameters(parameters)

        input_connector = InputConnector.from_list(parameters)
        super(ARMAmodel, self).__init__(input_connector, name)

    def _check_input(self, input_values):
        # Check whether input has correct type or format
        if len(input_values) != self.total_num_params:
            return False
        return True

    def _check_num_parameters(self, input_values):
        # Check whether input has correct type or format
        if len(input_values) != self.total_num_params:
            raise RuntimeError(
                'Input list must be of length {}, containing {} AR parameters and {} MA parameters'.format(
                    self.total_num_params, self.num_AR_params, self.num_MA_params))
        return True

    def _check_output(self, values):
        # f not isinstance(values[0], np.ndarray):
        #   raise ValueError('Output of the normal distribution is always a number.')
        return True

    def get_output_dimension(self):
        return self.size

    def get_number_parameters(self):
        return self.total_num_params

    def forward_simulate(self, input_values, k, rng=np.random.RandomState()):

        self._check_num_parameters(input_values)

        results = []
        arparams = np.array(input_values[0:self.num_AR_params])
        maparams = np.array(input_values[self.num_AR_params:self.num_AR_params + self.num_MA_params])
        ar = np.r_[1, -arparams.squeeze()]  # add zero-lag and negate
        ma = np.r_[1, maparams.squeeze()]  # add zero-lag

        for i in range(k):
            # use the random number generators provided as a way to draw the random gaussians. This is for
            # reproducibility. It does not work for statsmodels after 0.9.0
            results.append(arma_generate_sample(ar, ma, self.size, distrvs=rng.randn))

            if np.any(np.isinf(results[-1])):
                warnings.warn("Infinity in forward ARMA model", RuntimeWarning)
            if np.any(np.isnan(results[-1])):
                warnings.warn("nan in forward ARMA model", RuntimeWarning)

        return results


class MA2Reparametrized(ARMAmodel):

    def __init__(self, parameters, size=100, name='MA2Reparametrized'):
        """size is the length of the timeseries to be generated by the model. Parameter inputs to this model are
        R1 and R2 variables, from which theta1, theta2 are obtained.

        We in fact consider triangular region as specified in Marin et al (2012); we sample from it using the trick
        in https://math.stackexchange.com/questions/18686/uniform-random-point-in-triangle (implemented in
        self._transform_R_to_theta). With R1 and R2 uniform in [0,1], you obtain theta1, theta2 uniform on the correct
        triangular region.
        """

        self.size = size

        ARMAmodel.__init__(self, parameters, num_AR_params=0, num_MA_params=2, size=size, name=name)

    def forward_simulate(self, input_values, num_forward_simulations, rng=np.random.RandomState()):
        R1 = input_values[0]
        R2 = input_values[1]

        theta1, theta2 = self._transform_R_to_theta(R1, R2)

        return ARMAmodel.forward_simulate(self, [theta1, theta2], k=num_forward_simulations, rng=rng)

    def _check_input(self, input_values):
        """Check whether theta1 and theta2 from the transformation satisfy the inequalities:"""

        # first check number of parameters:
        if not ARMAmodel._check_input(self, input_values):
            return False

        # then check inequalities:
        R1 = input_values[0]
        R2 = input_values[1]

        theta1, theta2 = self._transform_R_to_theta(R1, R2)
        if not (-1 < theta2 < 1 and theta1 + theta2 > -1 or theta1 - theta2 < 1):
            return False

        return True

    @staticmethod
    def _transform_R_to_theta(R1, R2):
        R1_sqrt = R1 ** 0.5
        theta1 = ((4 - 2 * R2) * R1_sqrt - 2)
        theta2 = (1 - 2 * R2 * R1_sqrt)
        return theta1, theta2


class Multivariate_g_and_k(ProbabilisticModel, Continuous):
    """
    Multivariate g_and_k: a multivariate random normal with mean 0 and covariance matrix with 1 elements on the
    diagonal and rho on the upper and lower secondary diagonal is drawn. Then, the g-and-k transformation is
    applied to obtain the multivariate g-and-k.

    When using one single dimension, this is the standard g-and-k distribution.

    See for instance Section 4.5 of [1] for a discussion of the multivariate g-and-k in an ABC setup.

    This implementation was inspired by the R code at https://github.com/dennisprangle/gk, released under
    the GPL-2.0 License. The corresponding R package is discussed in [2], which also contains details on g-and-k
    distribution.

    [1] Jiang, Bai. "Approximate Bayesian computation with Kullback-Leibler divergence as data discrepancy."
    International Conference on Artificial Intelligence and Statistics. PMLR, 2018.
    [2] Prangle, Dennis. "gk: An R Package for the g-and-k and Generalised g-and-h Distributions."
    arXiv preprint arXiv:1706.06889 (2017), url https://arxiv.org/abs/1706.06889.
    """

    def __init__(self, parameters, size=5, name='Multivariate_g_and_k'):

        self.size = size
        self.c = 0.8  # fix this
        input_parameters = InputConnector.from_list(parameters)
        super(Multivariate_g_and_k, self).__init__(input_parameters, name)

    def forward_simulate(self, input_values, num_forward_simulations, rng=np.random.RandomState()):
        A = input_values[0]
        B = input_values[1]
        g = input_values[2]
        k = input_values[3]
        rho = input_values[4]

        result = self._draw_multiv_g_and_k(num_forward_simulations, self.size, A, B, g, k, rho, c=self.c, rng=rng)

        return [x for x in result]

    def _create_cov_matrix(self, dim, rho):
        return np.eye(dim) + rho * np.eye(dim, k=1) + rho * np.eye(dim, k=-1)

    def _draw_multiv_g_and_k(self, n, dim, A, B, g, k, rho, c=0.8, rng=np.random.RandomState()):
        """n is the number of samples, dim is the dimensions"""
        # define the covariance matrix first:

        cov = self._create_cov_matrix(dim, rho)

        z = rng.multivariate_normal(mean=np.zeros(dim), cov=cov, size=n)
        return self._z2gk(z, A, B, g, k, c=c)

    @staticmethod
    def _z2gk(z, A, B, g, k, c=0.8):
        """Transform a sample from a standard normal (z) to a g-and-k draw. This assumes z, A, B, g, k and c are
        scalars, while z can be an array. This is basically the Q function (the inverse of cdf) in the standard
        notation"""

        z = np.atleast_1d(z)
        z_squared = z ** 2
        if g == 0:
            term1 = 1
        else:
            term1 = (1 + c * np.tanh(g * z * 0.5))
        term2 = np.zeros_like(z, dtype=float)
        zbig = np.isinf(z_squared)
        zsmall = np.logical_not(zbig)
        term2[zbig] = np.sign(z[zbig]) * np.abs(z[zbig]) ** (1 + 2 * k)
        term2[zsmall] = z[zsmall] * (1 + z_squared[zsmall]) ** k

        return A + B * term1 * term2

    @staticmethod
    def _Q_log_derivative(z, A, B, g, k, c=0.8):
        """Compute the derivative of the Q function (the inverse cumulative function) evaluated at z."""

        z = np.atleast_1d(z)
        z_squared = z ** 2

        # These are used to correct edge cases
        # (likely to be rare so no need for particularly efficient code)
        zbig = np.isinf(z_squared)
        zsmall = np.logical_not(zbig)
        term1 = np.zeros_like(z, dtype=float)
        if k == 0:
            term1 = 0
        else:
            term1[zsmall] = k * np.log(1 + z_squared[zsmall])
            term1[zbig] = 2 * k * np.log(abs(z[zbig]))
        if g == 0:
            term2 = 1
            term4 = 0
        else:
            gz = g * z
            term2 = 1 + c * np.tanh(gz / 2)
            term4 = c * gz / (2 * np.cosh(gz / 2) ** 2)

        term3 = np.zeros_like(z, dtype=float)
        term3[zsmall] = (1 + (2 * k + 1) * z_squared[zsmall]) / (1 + z_squared[zsmall])
        term3[zbig] = 2 * k + 1
        # term4[ is.infinite(z)] = 0

        return np.log(B) + term1 + np.log(term2 * term3 + term4)

    def get_output_dimension(self):
        return self.size

    def _check_input(self, input_values):
        """
        It is standard to take B > 0 and fix c = 0.8, and in this case k ≥ 0 guarantees a proper distribution.
        """
        if len(input_values) != 5:
            return False
        A = input_values[0]
        B = input_values[1]
        g = input_values[2]
        k = input_values[3]
        rho = input_values[4]

        if not (-1 <= rho <= 1):
            return False
        if B <= 0 or k < 0:
            return False

        return True

    def _check_output(self, values):
        return True

    def logpdf(self, x, input_values, **kwargs_fsolve):
        """x here can be a single (multivariate) observation (1d array) or a set of observations (2d) array, with
         second index denoting the components"""
        if len(x.shape) not in (1, 2):
            raise RuntimeError("Incorrect number of components for x")

        A = input_values[0]
        B = input_values[1]
        g = input_values[2]
        k = input_values[3]
        rho = input_values[4]

        # first: solve the equation x_j = Q(z_j; theta) for z_j, for all components of x; this requires solving
        # numerically. Check how they do in g and k package.
        z = np.zeros_like(x.flatten())
        for j, x_j in enumerate(x.flatten()):
            func = lambda z: self._z2gk(z, A, B, g, k, self.c) - x_j

            z[j] = fsolve(func, x0=np.array([0]), **kwargs_fsolve)
        z = z.reshape(x.shape)

        # could also do it with one single call -> faster. Not sure if that works however in the same way, as the
        # scaling may be dependent on the dimension

        # func = lambda z: self._z2gk(z, A, B, g, k, self.c) - x
        # z = fsolve(func, x0=np.zeros_like(x), args=())
        # print(np.allclose(z, z2))

        # second: compute the log pdf by using the multivariate normal pdf and the derivative of the quantile function
        # evaluated in z_j
        cov = self._create_cov_matrix(self.size, rho)
        logpdf_mvn = multivariate_normal.logpdf(z, mean=np.zeros(self.size), cov=cov, allow_singular=True)

        return np.sum(logpdf_mvn) - np.sum(self._Q_log_derivative(z, A, B, g, k, self.c))


class Univariate_g_and_k(Multivariate_g_and_k):
    """Alias the multivariate one for the univariate case """

    def __init__(self, parameters, name='Univariate_g_and_k'):
        rho = 0  # unused
        super(Univariate_g_and_k, self).__init__(parameters + [rho], size=1, name=name)

    def forward_simulate(self, input_values, num_forward_simulations, rng=np.random.RandomState()):
        rho = 0  # this value is needed only for the parent method but unused with size=1
        return super(Univariate_g_and_k, self).forward_simulate(input_values + [rho], num_forward_simulations,
                                                                rng=rng)


class MG1Queue(ProbabilisticModel, Continuous):
    """Simulates a M/G/1 queueing system with Uni[theta1, theta2] service times and Exp(theta3) interarrival times.
    It returns the interdeparture time for the first number_steps steps, assuming the queue starts empty.

    [1] Nelson, Barry. Foundations and methods of stochastic simulation: a first course. Springer Science & Business
    Media, 2013.
    """

    def __init__(self, parameters, number_steps=5, name='M/G/1'):

        self.number_steps = number_steps
        input_parameters = InputConnector.from_list(parameters)
        super(MG1Queue, self).__init__(input_parameters, name)

    def forward_simulate(self, input_values, num_forward_simulations, rng=np.random.RandomState()):
        theta1 = input_values[0]
        theta2 = input_values[1]
        theta3 = input_values[2]

        simulations = self.simulate_mg1_vectorized(theta1, theta2, theta3, rng, num_forward_simulations)
        result = [None] * num_forward_simulations
        for i in range(num_forward_simulations):
            result[i] = simulations[:, i]

        return result

    def simulate_mg1_single(self, theta1, theta2, theta3, rng):
        # use here Lindley equation (notation follows chapter 4.3 of [1]) .
        Y = np.zeros(self.number_steps + 1)
        X = np.zeros(self.number_steps + 1)
        A = np.zeros(self.number_steps + 1)
        inter_dep = np.zeros(self.number_steps + 1)

        for i in range(1, self.number_steps + 1):
            A[i] = rng.exponential(scale=1 / theta3)  # scale is 1/rate
            X[i] = rng.uniform(theta1, theta2)
            Y[i] = np.max([0, Y[i - 1] + X[i - 1] - A[i]])
            # compute the inter-departure times:
            inter_dep[i] = A[i] + X[i] + Y[i] - Y[i - 1] - X[i - 1]
        # print(Y)
        return inter_dep[1:]

    def simulate_mg1_vectorized(self, theta1, theta2, theta3, rng, num_forward_simulations=1):
        # use here Lindley equation (notation follows chapter 4.3 of [1]) .
        Y = np.zeros((self.number_steps + 1, num_forward_simulations))
        X = np.zeros((self.number_steps + 1, num_forward_simulations))
        A = np.zeros((self.number_steps + 1, num_forward_simulations))
        inter_dep = np.zeros((self.number_steps + 1, num_forward_simulations))

        for i in range(1, self.number_steps + 1):
            A[i] = rng.exponential(scale=1 / theta3, size=num_forward_simulations)  # scale is 1/rate
            X[i] = rng.uniform(theta1, theta2, size=num_forward_simulations)
            Y[i] = np.clip(Y[i - 1] + X[i - 1] - A[i], a_min=0, a_max=np.infty)  # clip from below with 0
            # compute the inter-departure times:
            inter_dep[i] = A[i] + X[i] + Y[i] - Y[i - 1] - X[i - 1]
        return inter_dep[1:]

    def get_output_dimension(self):
        return self.number_steps

    def _check_input(self, input_values):
        """
        """
        if len(input_values) != 3:
            return False

        if input_values[0] < 0 or input_values[1] <= 0 or input_values[2] <= 0 or input_values[0] > input_values[1]:
            return False
        return True

    def _check_output(self, values):
        return True


class MG1QueueReparametrized(MG1Queue):
    """This takes as input theta1, theta2-theta1, theta3 and then calls the MG1Queue model"""

    def forward_simulate(self, input_values, num_forward_simulations, rng=np.random.RandomState()):
        theta1 = input_values[0]
        theta2_minus_theta1 = input_values[1]
        theta3 = input_values[2]

        theta2 = theta2_minus_theta1 + theta1

        return MG1Queue.forward_simulate(self, [theta1, theta2, theta3], num_forward_simulations, rng=rng)

    def _check_input(self, input_values):
        """
        """
        theta1 = input_values[0]
        theta2_minus_theta1 = input_values[1]
        theta3 = input_values[2]

        return MG1Queue._check_input(self, [theta1, theta2_minus_theta1 + theta1, theta3])


class LogStatistics(Statistics):
    """Element-wise log of the data
    """

    def __init__(self):
        pass

    def statistics(self, data):
        if isinstance(data, list):
            if np.array(data).shape == (len(data),):
                if len(data) == 1:
                    data = np.array(data).reshape(1, 1)
                data = np.array(data).reshape(len(data), 1)
            else:
                data = np.concatenate(data).reshape(len(data), -1)
        else:
            raise TypeError('Input data should be of type list, but found type {}'.format(type(data)))

        return np.log(data)


class StochLorenz96(ProbabilisticModel, Continuous):
    """Generates time dependent 'slow' weather variables following forecast model of Wilks [1],
        a stochastic reparametrization of original Lorenz model Lorenz [2].

        [1] Wilks, D. S. (2005). Effects of stochastic parametrizations in the lorenz ’96 system.
        Quarterly Journal of the Royal Meteorological Society, 131(606), 389–407.

        [2] Lorenz, E. (1995). Predictability: a problem partly solved. In Proceedings of the
        Seminar on Predictability, volume 1, pages 1–18. European Center on Medium Range
        Weather Forecasting, Europe

        Parameters
        ----------
        parameters: list
            Contains the closure parameters of the parametrization: [theta_1, theta_2, sigma_e, phi].
        initial_state: numpy.ndarray, optional
            Initial state value of the time-series, The default value is None, which assumes a previously computed
            value from a full Lorenz model as the Initial value.
        F, b, h, c: floats, optional
            These represent some constants used in the definition of the ODEs.
        K: integer, optional
            The total number of slow variables
        J: integer, optional
            The number of fast variables for each slow variable
        time_units: float, optional
            Number of time unites for which to run the system; default to 4, whih corresponds roughly to 20 days.
        n_timestep_per_time_unit: int, optional
            Number of steps for a time unit. The default value is 30 steps.
        name: string, optional
            Name of the model
        """

    def __init__(self, parameters, initial_state=None, F=10, b=10, h=1, c=4, J=8, K=40, time_units=4,
                 n_timestep_per_time_unit=30, name='StochLorenz96'):
        # settings in Arnold et al. K=8, J=32, h=1, F=20, b=10, c=10 or c=4
        self._set_initial_state(initial_state)
        self.F = F  # sometimes this parameter is given value 20; it is given value 10 in Hakkarainen et al. (2012);
        # either 18 or 20 in Wilks (2005); 20 in Arnold et al. (2013)
        # self.sigma_e = 1
        # self.phi = 0.4

        # define the parameters of the true model:
        self.b = b
        self.h = h
        self.c = c  # 10 is also used sometimes; it corresponds to the easy case, while c=4 is harder.
        self.J = J
        self.K = K
        self.time_units = time_units
        self.n_timestep_per_time_unit = n_timestep_per_time_unit
        self.n_timestep = int(self.time_units * self.n_timestep_per_time_unit)
        self.hc_over_b = self.h * self.c / self.b
        self.cb = self.c * self.b

        if not isinstance(parameters, list):
            raise TypeError('Input of StochLorenz96 model is of type list')

        if len(parameters) != 4:
            raise RuntimeError('Input list must be of length 4, containing [theta1, theta2, sigma_e, phi].')

        input_connector = InputConnector.from_list(parameters)
        super().__init__(input_connector, name)

    def _set_initial_state(self, initial_state, lam=0):
        """This sets the initial state to a fixed value (if that was not provided). lam is the magnitude of the gaussian
        noise that is added to the initial state; it is used only in the Ensemble model."""
        if initial_state is None:
            # Assign initial state; this has 40 variables, it is suitable for K=40. In case K<40 is used, the initial
            # state is considered to be self.initial_state[0:K]
            self.initial_state = np.array([6.4558, 1.1054, -1.4502, -0.1985, 1.1905, 2.3887, 5.6689, 6.7284, 0.9301,
                                           4.4170, 4.0959, 2.6830, 4.7102, 2.5614, -2.9621, 2.1459, 3.5761, 8.1188,
                                           3.7343, 3.2147, 6.3542, 4.5297, -0.4911, 2.0779, 5.4642, 1.7152, -1.2533,
                                           4.6262, 8.5042, 0.7487, -1.3709, -0.0520, 1.3196, 10.0623, -2.4885,
                                           -2.1007, 3.0754, 3.4831, 3.5744, 6.5790])
        else:
            self.initial_state = initial_state
        if lam > 0:
            self.initial_state += np.random.normal(scale=lam, size=self.initial_state.shape)

    def _check_input(self, input_values):
        # Check whether input has correct type or format
        if len(input_values) != 4:
            raise ValueError('Number of parameters of StochLorenz96 model must be 4.')

        # Check whether input is from correct domain
        theta1 = input_values[0]
        theta2 = input_values[1]
        sigma_e = input_values[2]
        phi = input_values[3]

        # if theta1 <= 0 or theta2 <= 0:
        # why? this does not make much sense, the parameters of the deterministic part could be smaller than 0
        #    return False

        if sigma_e < 0 or phi < 0 or phi > 1:
            return False

        return True

    def _check_output(self, values):
        # if not isinstance(values[0], np.ndarray):
        #     raise ValueError('Output of the normal distribution is always a number.')
        return True

    def get_output_dimension(self):
        return self.K * self.n_timestep

    def get_number_parameters(self):
        return 4

    def forward_simulate(self, input_values, k, rng=np.random.RandomState()):
        # Extract the input parameters
        theta1 = input_values[0]
        theta2 = input_values[1]
        sigma_e = input_values[2]
        phi = input_values[3]

        # Do the actual forward simulation
        vector_of_k_samples = self.Lorenz95(theta1, theta2, sigma_e, phi, k, rng=rng)
        # Format the output to obey API
        result = [np.array([x]) for x in vector_of_k_samples]
        return result

    def forward_simulate_true_model(self, k, n_timestep_per_time_unit=None, rng=np.random.RandomState()):
        # Do the actual forward simulation
        vector_of_k_samples_x, vector_of_k_samples_y = self.Lorenz95True(k,
                                                                         n_timestep_per_time_unit=n_timestep_per_time_unit,
                                                                         rng=rng)
        # Format the output to obey API
        result_x = [np.array([x]) for x in vector_of_k_samples_x]
        result_y = [np.array([y]) for y in vector_of_k_samples_y]
        return result_x, result_y

    def Lorenz95(self, theta1, theta2, sigma_e, phi, k, rng=np.random.RandomState()):

        # Generate n_simulate time-series of weather variables satisfying Lorenz 95 equations
        result = []

        # Initialize timesteps.
        time_steps = np.linspace(0, self.time_units, self.n_timestep)

        for k in range(0, k):
            # Define a parameter object containing parameters which is needed
            # to evaluate the ODEs
            # Stochastic forcing term
            eta = sigma_e * np.sqrt(1 - pow(phi, 2)) * rng.normal(0, 1, self.K)

            # Initialize the time-series
            timeseries = np.zeros(shape=(self.K, self.n_timestep), dtype=np.float)
            timeseries[:, 0] = self.initial_state[0:self.K]
            # Compute the timeseries for each time steps
            for ind in range(0, self.n_timestep - 1):
                # parameters to be supplied to the ODE solver
                parameter = [eta, np.array([theta1, theta2])]
                # Each timestep is computed by using a 4th order Runge-Kutta solver
                x = self._rk4ode(self._l95ode_par, np.array([time_steps[ind], time_steps[ind + 1]]),
                                 timeseries[:, ind],
                                 parameter)
                timeseries[:, ind + 1] = x[:, -1]
                # Update stochastic forcing term
                eta = phi * eta + sigma_e * np.sqrt(1 - pow(phi, 2)) * rng.normal(0, 1, self.K)
            result.append(timeseries.flatten())
        # return an array of objects of type Timeseries
        return result

    def Lorenz95True(self, k, n_timestep_per_time_unit=None, rng=np.random.RandomState()):
        """"Note that here there is randomness in the choice of the starting value of the y variables. I chose them to
        be uniform in [0,1] at the beginning. """

        if n_timestep_per_time_unit is None:
            n_timestep = self.n_timestep
        else:
            n_timestep = int(self.time_units * n_timestep_per_time_unit)
        # Generate n_simulate time-series of weather variables satisfying Lorenz 95 equations
        result_X = []
        result_Y = []

        # Initialize timesteps
        # it is better to use a smaller timestep for the true model, as the solver may diverge otherwise.
        time_steps = np.linspace(0, self.time_units, n_timestep)

        # define the initial state of the Y variables. We take self.J fast variables per slow variable
        self.initial_state_Y = rng.uniform(size=(self.K * self.J))

        for k in range(0, k):
            # Define a parameter object containing parameters which is needed
            # to evaluate the ODEs

            # Initialize the time-series
            timeseries_X = np.zeros(shape=(self.K, n_timestep), dtype=np.float)
            timeseries_X[:, 0] = self.initial_state[0:self.K]

            timeseries_Y = np.zeros(shape=(self.initial_state_Y.shape[0], n_timestep), dtype=np.float)
            timeseries_Y[:, 0] = self.initial_state_Y

            # Compute the timeseries for each time steps
            # the loop would not be needed if we wrote a single ode function for both set of variables.
            for ind in range(0, n_timestep - 1):
                # Each timestep is computed by using a 4th order Runge-Kutta solver
                x = self._rk4ode(self._l95ode_true_X, np.array([time_steps[ind], time_steps[ind + 1]]),
                                 timeseries_X[:, ind],
                                 [timeseries_Y[:,
                                  ind]])  # we pass the value of the other set of variables as the parameter.
                y = self._rk4ode(self._l95ode_true_Y, np.array([time_steps[ind], time_steps[ind + 1]]),
                                 timeseries_Y[:, ind],
                                 [timeseries_X[:,
                                  ind]])  # we pass the value of the other set of variables as the parameter.

                timeseries_X[:, ind + 1] = x[:, -1]
                timeseries_Y[:, ind + 1] = y[:, -1]

            result_X.append(timeseries_X.flatten())
            result_Y.append(timeseries_Y.flatten())

        # return an array of objects of type Timeseries
        return result_X, result_Y

    def _l95ode_par(self, t, x, parameter):
        """
        The parameterized two-tier lorenz 95 system defined by a set of symmetic
        ordinary differential equation. This function evaluates the differential
        equations at a value x of the time-series

        Parameters
        ----------
        x: numpy.ndarray of dimension px1
            The value of timeseries where we evaluate the ODE
        parameter: Python list
            The set of parameters needed to evaluate the function
        Returns
        -------
        numpy.ndarray
            Evaluated value of the ode at a fixed timepoint
        """
        # Initialize the array containing the evaluation of ode
        dx = np.zeros(shape=(x.shape[0],))
        eta = parameter[0]
        theta = parameter[1]
        # Deterministic parameterization for fast weather variables
        # ---------------------------------------------------------
        # assumed to be polynomial, degree of the polynomial same as the
        # number of columns in closure parameter
        degree = theta.shape[0]
        X = np.ones(shape=(x.shape[0], 1))
        for ind in range(1, degree):
            X = np.column_stack((X, pow(x, ind)))

        # deterministic reparametrization term
        # ------------------------------------
        gu = np.sum(X * theta, 1)

        # ODE definition of the slow variables
        # ------------------------------------
        dx[0] = -x[-2] * x[-1] + x[-1] * x[1] - x[0] + self.F - gu[0] + eta[0]
        dx[1] = -x[-1] * x[0] + x[0] * x[2] - x[1] + self.F - gu[1] + eta[1]
        for ind in range(2, x.shape[0] - 1):
            dx[ind] = -x[ind - 2] * x[ind - 1] + x[ind - 1] * x[ind + 1] - x[ind] + self.F - gu[ind] + eta[ind]
        dx[-1] = -x[-3] * x[-2] + x[-2] * x[1] - x[-1] + self.F - gu[-1] + eta[-1]

        return dx

    def _l95ode_true_X(self, t, x, parameter):
        """
        The parameterized two-tier lorenz 95 system defined by a set of symmetic
        ordinary differential equation. This function evaluates the differential
        equations at a value x of the time-series

        Parameters
        ----------
        x: numpy.ndarray of dimension px1
            The value of timeseries where we evaluate the ODE
        parameter: Python list
            The set of parameters needed to evaluate the function
        Returns
        -------
        numpy.ndarray
            Evaluated value of the ode at a fixed timepoint
        """
        # Initialize the array containing the evaluation of ode
        dx = np.zeros(shape=(x.shape[0],))
        y = parameter[0]

        # ODE definition of the slow variables
        # ------------------------------------

        dx[0] = -x[-2] * x[-1] + x[-1] * x[1] - x[0] + self.F - self.hc_over_b * np.sum(y[0: self.J])
        dx[1] = -x[-1] * x[0] + x[0] * x[2] - x[1] + self.F - self.hc_over_b * np.sum(y[self.J: 2 * self.J])
        for ind in range(2, x.shape[0] - 1):
            dx[ind] = -x[ind - 2] * x[ind - 1] + x[ind - 1] * x[ind + 1] - x[ind] + self.F - \
                      self.hc_over_b * np.sum(y[self.J * ind: self.J * (ind + 1)])
        dx[-1] = -x[-3] * x[-2] + x[-2] * x[1] - x[-1] + self.F - self.hc_over_b * np.sum(y[-1 * self.J:])

        return dx

    def _l95ode_true_Y(self, t, y, parameter):
        """
        The parameterized two-tier lorenz 95 system defined by a set of symmetic
        ordinary differential equation. This function evaluates the differential
        equations at a value x of the time-series

        Parameters
        ----------
        y: numpy.ndarray of dimension px1
            The value of timeseries where we evaluate the ODE
        parameter: Python list
            The set of parameters needed to evaluate the function
        Returns
        -------
        numpy.ndarray
            Evaluated value of the ode at a fixed timepoint
        """
        # Initialize the array containing the evaluation of ode
        dy = np.zeros(shape=(y.shape[0],))
        x = parameter[0]

        # ODE definition of the fast variables
        # ------------------------------------
        for ind in range(y.shape[0] - 2):
            dy[ind] = - self.cb * y[ind + 1] * (y[ind + 2] - y[ind - 1]) - self.c * y[ind] + \
                      self.hc_over_b * x[ind // self.J]  # // for the integer division.

        dy[-2] = - self.cb * y[- 1] * (y[0] - y[-3]) - self.c * y[-2] + \
                 self.hc_over_b * x[-2 // self.J]  # // for the integer division.

        dy[-1] = - self.cb * y[0] * (y[1] - y[-2]) - self.c * y[-1] + \
                 self.hc_over_b * x[-1 // self.J]  # // for the integer division.

        return dy

    def _rk4ode(self, ode, timespan, timeseries_initial, parameter):
        """
        4th order runge-kutta ODE solver.

        Parameters
        ----------
        ode: function
            The function defining Ordinary differential equation
        timespan: numpy.ndarray
            A numpy array containing the timepoints where the ode needs to be solved.
            The first time point corresponds to the initial value
        timeseries_initial: np.ndarray of dimension px1
            Intial value of the time-series, corresponds to the first value of timespan
        parameter: Python list
            The parameters needed to evaluate the ode
        Returns
        -------
        np.ndarray
            Timeseries initiated at timeseries_init and satisfying ode solved by this solver.
        """

        timeseries = np.zeros(shape=(timeseries_initial.shape[0], timespan.shape[0]))
        timeseries[:, 0] = timeseries_initial

        for ind in range(0, timespan.shape[0] - 1):
            time_diff = timespan[ind + 1] - timespan[ind]
            time_mid_point = timespan[ind] + time_diff / 2
            k1 = time_diff * ode(timespan[ind], timeseries_initial, parameter)
            k2 = time_diff * ode(time_mid_point, timeseries_initial + k1 / 2, parameter)
            k3 = time_diff * ode(time_mid_point, timeseries_initial + k2 / 2, parameter)
            k4 = time_diff * ode(timespan[ind + 1], timeseries_initial + k3, parameter)
            timeseries_initial = timeseries_initial + (k1 + 2 * k2 + 2 * k3 + k4) / 6
            timeseries[:, ind + 1] = timeseries_initial
        # Return the solved timeseries at the values in timespan
        return timeseries


class LorenzLargerStatistics(Statistics):

    def __init__(self, moments=3, auto_covariances=5, covariances=5, cross_covariances=5, degree=1, cross=False, K=8):
        """We compute here moments, covariances, cross covariances, autocovariances."""
        self.number_moments = moments
        self.number_auto_covariances = auto_covariances
        self.number_covariances = covariances
        self.number_cross_covariances = cross_covariances
        self.degree = degree
        self.cross = cross
        self.K = K  # number of variables

    def statistics(self, data):
        if isinstance(data, list):
            if np.array(data).shape == (len(data),):
                if len(data) == 1:
                    data = np.array(data).reshape(1, 1)
                data = np.array(data).reshape(len(data), 1)
            else:
                data = np.concatenate(data).reshape(len(data), -1)
        else:
            raise TypeError('Input data should be of type list, but found type {}'.format(type(data)))
        # Extract Summary Statistics
        num_element, timestep = len(data), int(data[0].shape[0] / self.K)
        result = np.zeros(shape=(num_element, self.number_moments + self.number_auto_covariances +
                                 self.number_covariances + 2 * self.number_cross_covariances))
        # Compute statistics
        if np.any(np.isinf(data)):
            warnings.warn("Infinity in the output of Lorenz95 model", RuntimeWarning)
        if np.any(np.isnan(data)):
            warnings.warn("nan in the output of Lorenz95 model", RuntimeWarning)

        for ind_element in range(0, num_element):
            # First convert the vector to the self.K dimensional timeseries
            data_ind_element = data[ind_element].reshape(self.K, timestep)
            # compute 10 central moments:
            moments = np.zeros(self.number_moments)
            moments[0] = np.mean(np.mean(data_ind_element, 1))
            for i in range(1, self.number_moments):
                moments[i] = np.mean(moment(data_ind_element, axis=1, moment=i + 1))
                # second centered moment corresponds to variance

            # auto-covariances: covariance between one timeseries and itself with some lag
            # Here we repeat computation, as we could return the auto-covariances for all lags with
            # one single function call. It does not matter too much for now.
            auto_covariances = np.zeros(self.number_auto_covariances)
            for ind in range(0, data_ind_element.shape[0]):  # loop over the different components of timeseries.
                for i in range(self.number_auto_covariances):
                    auto_covariances[i] += self._covariance_lag(data_ind_element[ind, :],
                                                                data_ind_element[ind, :], lag=i + 1)
            auto_covariances /= data_ind_element.shape[0]

            # covariances between different timeseries with lag 0 and spatial distance up to 5, with cyclic
            # boundary conditions.
            covariances_neighbors = np.zeros(self.number_covariances)
            for ind in range(0, data_ind_element.shape[0]):  # loop over the different components of timeseries.
                for i in range(self.number_covariances):
                    # the % is for the cyclic conditions
                    covariances_neighbors[i] += self._covariance_lag(
                        data_ind_element[ind, :], data_ind_element[(ind + i + 1) % data_ind_element.shape[0], :], lag=0)
            covariances_neighbors /= data_ind_element.shape[0]

            # we now extract cross-covariances with both time lag and spatial distance: consider timeseries at distance
            # i and use lag i, by keeping them to be the same:
            cross_covariances_neighbors_right = np.zeros(self.number_cross_covariances)
            cross_covariances_neighbors_left = np.zeros(self.number_cross_covariances)
            for ind in range(0, data_ind_element.shape[0]):  # loop over the different components of timeseries.
                for i in range(self.number_cross_covariances):
                    # the % is for the cyclic conditions
                    cross_covariances_neighbors_right[i] += self._covariance_lag(
                        data_ind_element[ind, :], data_ind_element[(ind + i + 1) % data_ind_element.shape[0], :],
                        lag=i + 1)
                    cross_covariances_neighbors_left[i] += self._covariance_lag(
                        data_ind_element[(ind + i + 1) % data_ind_element.shape[0], :], data_ind_element[ind, :],
                        lag=i + 1)
            cross_covariances_neighbors_right /= data_ind_element.shape[0]
            cross_covariances_neighbors_left /= data_ind_element.shape[0]

            result[ind_element] = np.concatenate((moments, auto_covariances, covariances_neighbors,
                                                  cross_covariances_neighbors_right, cross_covariances_neighbors_left))

            for i, stat in enumerate(result[ind_element]):
                if np.any(np.isinf(stat)):
                    warnings.warn("Infinity in stat number {}".format(i + 1), RuntimeWarning)
                if np.any(np.isnan(stat)):
                    warnings.warn("nan in stat number {}".format(i + 1), RuntimeWarning)
            # Expand the data with polynomial expansion
        result = self._polynomial_expansion(result)
        return np.array(result)

    def _covariance_lag(self, x, y, lag=0):
        """ Computes cross-covariance between x and y. Using np.correlate is faster than using the for loop over
        elements. This function can be used to compute all kinds of covariances/autocovariancess/cross-covariances
        needed.

        Parameters
        ----------
        x: numpy.ndarray
            Vector of real numbers.
        y: numpy.ndarray
            Vector of real numbers.

        Returns
        -------
        numpy.ndarray
            Cross-covariance calculated between x and y.
        """
        assert x.shape[0] == y.shape[0]  # we consider case in which the timeseries all have same length
        # data_size = np.int(x.shape[0] / 2)  # it may work only for timeseries with even number of timesteps.
        # the proper definition of covariance also subtracts the means of the two series that you are considering.
        if lag == 0:
            return np.correlate(x, y, mode="valid")[0] / x.shape[0] - np.mean(x) * np.mean(y)
        else:
            return np.correlate(x[lag:], y[:-lag], mode="valid")[0] / (x.shape[0] - lag) - np.mean(x[lag:]) * np.mean(
                y[:-lag])
            # this in case you want to return different lags at once; don't care about that for now.
            # return np.correlate(x, y, mode="same")[data_size + 1: data_size + self.order + 1]  # many correlations at once
            # return np.correlate(x, y, mode="same")[data_size + lag] / (x.shape[0] - lag) - np.mean(x[lag:]) * np.mean(
            #     y[:-lag] if lag > 0 else y)  # x[:-0] does not work


class HakkarainenLorenzStatistics(LorenzLargerStatistics):
    """
    This class implements the statistics function from the Statistics protocol. This
    extracts the statistics following Hakkarainen et. al. [1] from the multivariate timesereis
    generated by solving Lorenz 95 odes.

    [1] J. Hakkarainen, A. Ilin, A. Solonen, M. Laine, H. Haario, J. Tamminen, E. Oja, and
    H. Järvinen. On closure parameter estimation in chaotic systems. Nonlinear Processes
    in Geophysics, 19(1):127–143, Feb. 2012.
    """

    def __init__(self, degree=2, cross=True, K=8):
        self.degree = degree
        self.cross = cross
        self.K = K

        super(HakkarainenLorenzStatistics, self).__init__(moments=2, auto_covariances=1, covariances=1,
                                                          cross_covariances=1, degree=degree, cross=cross, K=K)


class RecruitmentBoomBust(ProbabilisticModel, Continuous):
    """Recruitment, Boom and Bust model as described originally in [1]. It is a discrete stochastic temporal
    model used to represent the fluctuation of population size over time.

    [1] Fasiolo, M., Wood, S. N., Hartig, F., & Bravington, M. V. (2018). An extended empirical saddlepoint
    approximation for intractable likelihoods. Electronic Journal of Statistics, 12(1), 1544-1578.

    Parameters
    ----------
    parameters: list
        Contains the probabilistic models and hyperparameters from which the model derives.
    n_timestep: int, optional
        Number of timesteps. The default value is 250.
    burnin: int, optional
        Number of burnin timesteps to discard transient. The default value is 50.
    """

    def __init__(self, parameters, n_timestep=250, burnin=50, name="RecruitmentBoomBust"):
        input_parameters = InputConnector.from_list(parameters)

        self.n_timestep = n_timestep
        self.burnin = burnin
        # Parameter specifying the dimension of the return values of the distribution.
        super(RecruitmentBoomBust, self).__init__(input_parameters, name)

    def forward_simulate(self, input_values, k, rng=np.random.RandomState()):
        timeseries_array = [None] * k
        # Initialize local parameters
        r = input_values[0]
        kappa = input_values[1]
        alpha = input_values[2]
        beta = input_values[3]

        r_plus_1 = 1 + r

        # for i in range(0, k):
        #     # Initialize the time-series
        #     timeseries = np.zeros(shape=self.n_timestep + self.burnin + 1, dtype=np.int)
        #     timeseries[0] = kappa  # initial condition is k
        #     epsilon_t = rng.poisson(beta, self.n_timestep + self.burnin)
        #     for ind in range(1, self.n_timestep + self.burnin + 1):
        #         if timeseries[ind - 1] <= kappa:
        #             timeseries[ind] = rng.poisson(timeseries[ind - 1] * r_plus_1) + epsilon_t[ind - 1]
        #         else:
        #             timeseries[ind] = rng.binomial(timeseries[ind - 1], alpha) + epsilon_t[ind - 1]
        #
        #     timeseries_array[i] = timeseries[self.burnin + 1:]
        #

        timeseries = np.zeros(shape=(self.n_timestep + self.burnin + 1, k), dtype=np.int)
        timeseries[0] = kappa  # initial condition is k
        epsilon_t = rng.poisson(beta, (self.n_timestep + self.burnin, k))
        for ind in range(1, self.n_timestep + self.burnin + 1):
            # find where the timeseries is <= kappa:
            index_smaller_kappa = timeseries[ind - 1] <= kappa
            index_larger_kappa = np.logical_not(index_smaller_kappa)
            n_index_smaller_kappa = np.sum(index_smaller_kappa)
            timeseries[ind, index_smaller_kappa] = \
                rng.poisson(timeseries[ind - 1, index_smaller_kappa] * r_plus_1, size=n_index_smaller_kappa) + \
                epsilon_t[ind - 1, index_smaller_kappa]
            timeseries[ind, index_larger_kappa] = \
                rng.binomial(timeseries[ind - 1, index_larger_kappa], alpha, size=k - n_index_smaller_kappa) + \
                epsilon_t[ind - 1, index_larger_kappa]

        for i in range(0, k):
            timeseries_array[i] = timeseries[self.burnin + 1:, i]

        return timeseries_array

    def _check_input(self, input_values):
        """
        """
        if len(input_values) != 4:
            return False
        r = input_values[0]
        kappa = input_values[1]
        alpha = input_values[2]
        beta = input_values[3]

        if r <= 0 or beta <= 0 or alpha < 0 or alpha > 1 or kappa < 0:
            return False
        return True

    def _check_output(self, values):
        return True

    def get_output_dimension(self):
        return self.n_timestep


class RecruitmentBoomBustStatistics(Statistics):
    """Statistics for the Recruitment, Boom and Bust model, which was originally described in [1]. The statistics we
    implement here are the ones used in [2]

    [1] Fasiolo, M., Wood, S. N., Hartig, F., & Bravington, M. V. (2018). An extended empirical saddlepoint
    approximation for intractable likelihoods. Electronic Journal of Statistics, 12(1), 1544-1578.

    [2] An, Z., Nott, D. J., & Drovandi, C. (2020). Robust Bayesian synthetic likelihood via a semi-parametric approach.
    Statistics and Computing, 30(3), 543-557.
    """

    def __init__(self):
        pass

    def statistics(self, data):
        if isinstance(data, list):
            if np.array(data).shape == (len(data),):
                if len(data) == 1:
                    data = np.array(data).reshape(1, 1)
                data = np.array(data).reshape(len(data), 1)
            else:
                data = np.concatenate(data).reshape(len(data), -1)
        else:
            raise TypeError('Input data should be of type list, but found type {}'.format(type(data)))

        # need to compute the stepwise differences and the stepwise ratios:
        n_observations = data.shape[0]

        results = np.zeros((n_observations, 12))

        for obs_index in range(n_observations):
            differences = data[obs_index][1:] - data[obs_index][0:-1]
            ratios = (data[obs_index][1:] + 1) / (data[obs_index][0:-1] + 1)

            # now compute mean, variance, skewness and kurtosis; notice that, to be consistent with what is done in
            # An et al. 2020 [2], the kurtosis keeps factor 3 and the variance uses 1 ddof.
            results[obs_index] = [
                np.mean(data[obs_index]), np.var(data[obs_index], ddof=1), skew(data[obs_index]),
                kurtosis(data[obs_index], fisher=False),
                np.mean(ratios), np.var(ratios, ddof=1), skew(ratios), kurtosis(ratios, fisher=False),
                np.mean(differences), np.var(differences, ddof=1), skew(differences),
                kurtosis(differences, fisher=False)]
        return results


def instantiate_model(model_name, reparametrized=True, **kwargs):
    param_bounds = None
    statistics = Identity(degree=1, cross=False)

    if model_name == "MA2":
        if "size" in kwargs.keys():
            size = kwargs["size"]
        else:
            size = 50

        if reparametrized:
            R1 = Uniform([[0], [1]], name='R1')
            R2 = Uniform([[0], [1]], name='R2')

            model = MA2Reparametrized([R1, R2], size=size, **kwargs)
            # Notice however that here the journal stores R1 and R2, not theta1 and theta2! Need then to convert to
            # those in order to obtain the exact posterior samples...
            param_bounds = {"R1": [0, 1], "R2": [0, 1]}
        else:
            # instantiate the non-reparametrized version for simulations from a specified parameter value:
            par_values = define_exact_param_values()[model_name]

            theta1 = par_values[0]
            theta2 = par_values[1]

            model = ARMAmodel([theta1, theta2], num_AR_params=0, num_MA_params=2, size=size)

    elif model_name in ["g-and-k", "Cauchy_g-and-k"]:
        # prior ranges as in Jiang et al (2018) and Fujisawa et al (2020)
        A = Uniform([[0], [4]], name='A')
        B = Uniform([[0], [4]], name='B')
        g = Uniform([[0], [4]], name='g')
        k = Uniform([[0], [4]], name='k')
        rho_bound = np.sqrt(3) / 3
        rho = Uniform([[-rho_bound], [rho_bound]], name='rho')  # notice this had a wrong name

        model = Multivariate_g_and_k([A, B, g, k, rho], **kwargs)
        param_bounds = {'A': [0, 4], 'B': [0, 4], 'g': [0, 4], 'k': [0, 4], "rho": [-rho_bound, rho_bound]}

    elif model_name in ["univariate_g-and-k", "univariate_Cauchy_g-and-k"]:
        # prior ranges as in Jiang et al (2018) and Fujisawa et al (2020)
        A = Uniform([[0], [4]], name='A')
        B = Uniform([[0], [4]], name='B')
        g = Uniform([[0], [4]], name='g')
        k = Uniform([[0], [4]], name='k')
        rho_useless = 0

        model = Multivariate_g_and_k([A, B, g, k, rho_useless], size=1)
        param_bounds = {'A': [0, 4], 'B': [0, 4], 'g': [0, 4], 'k': [0, 4]}

    elif model_name == "MG1":
        # prior ranges as in Jiang et al (2018) and Fujisawa et al (2020)
        if reparametrized:
            # use the parametrized definition for inference routines
            theta1 = Uniform([[0], [10]], name='theta1')
            theta2_minus_theta1 = Uniform([[0], [10]], name='theta2_minus_theta1')  # theta2 - theta1 uniform in [0,10]
            theta3 = Uniform([[0], [1 / 3]], name='theta3')

            model = MG1QueueReparametrized([theta1, theta2_minus_theta1, theta3], number_steps=50, **kwargs)
            param_bounds = {'theta1': [0, 10], 'theta2_minus_theta1': [0, 10], 'theta3': [0, 1 / 3]}
        else:
            # instantiate the non-reparametrized version for simulations from a specified parameter value:
            par_values = define_exact_param_values()[model_name]

            theta1 = par_values[0]
            theta2 = par_values[1]
            theta3 = par_values[2]

            model = MG1Queue([theta1, theta2, theta3], number_steps=50, **kwargs)

        statistics = LogStatistics()

    elif model_name == "normal_location_misspec":

        theta = Normal([[0], [1]], name='theta')
        model = Normal([[theta], [1]], name='normal_location_misspec')

    elif "RecruitmentBoomBust" in model_name:
        # Prior ranges are the ones used in An et al 2020
        r = Uniform([[0], [1]], name='r')
        kappa = Uniform([[10], [80]], name='kappa')
        alpha = Uniform([[0], [1]], name='alpha')
        beta = Uniform([[0], [1]], name='beta')

        model = RecruitmentBoomBust([r, kappa, alpha, beta], **kwargs)
        statistics = RecruitmentBoomBustStatistics()

        param_bounds = {'r': [0, 1], 'kappa': [10, 80], 'alpha': [0, 1], 'beta': [0, 1]}

    elif "Lorenz96" in model_name:
        theta1_min = 1.4
        theta1_max = 2.2
        theta2_min = 0
        theta2_max = 1

        sigma_e_min = 1.5
        sigma_e_max = 2.5
        phi_min = 0
        phi_max = 1

        theta1 = Uniform([[theta1_min], [theta1_max]], name='theta1')
        theta2 = Uniform([[theta2_min], [theta2_max]], name='theta2')
        sigma_e = Uniform([[sigma_e_min], [sigma_e_max]], name='sigma_e')
        phi = Uniform([[phi_min], [phi_max]], name='phi')

        model = StochLorenz96([theta1, theta2, sigma_e, phi], time_units=1.5, n_timestep_per_time_unit=30, K=8,
                              name='lorenz')
        param_bounds = {'theta1': [theta1_min, theta1_max], 'theta2': [theta2_min, theta2_max],
                        'sigma_e': [sigma_e_min, sigma_e_max], 'phi': [phi_min, phi_max]}

    else:
        raise NotImplementedError

    return model, statistics, param_bounds
